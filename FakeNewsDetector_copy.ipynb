{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = input(\"ENter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"\\\"100% Tariffs From India, 50% From EU\\\": What White House Said On Trump's Liberation Day Plans\",\n",
      "    \"authors\": [],\n",
      "    \"publication_date\": \"Unknown\",\n",
      "    \"content\": \"Amid suspense over US President Donald Trump's \\\"Liberation Day tariff\\\" plans, White House spokesperson Karoline Leavitt confirmed on Monday that there won't be any exemptions on reciprocal tariffs. Listing tariffs imposed on American products by foreign nations, Ms Leavitt said that the \\\"unfair trade practices\\\" need to stop.\\n\\nThis comes as President Trump prepares to announce a round of reciprocal tariffs on April 2, a day he has labeled \\\"Liberation Day\\\" of America.\\n\\n\\\"The goal of Wednesday is a country-based tariff, but certainly sectoral tariffs. The President has said he's committed to implementing them, and I would leave it to him when he makes that announcement,\\\" Leavitt said, adding that \\\"It's time for reciprocity.\\\"\\n\\nHOLY SH*T \\ud83d\\udea8 Karoline Leavitt brings a chart and shuts down Fake News Media with Tariff Facts:\\n\\n\\n\\n- 700% tariff from Japan on rice\\n\\n- 50% tariff from the EU on American dairy\\n\\n- 100% tariff from India on agricultural products\\n\\n\\n\\nWE COME WITH RECEIPTS pic.twitter.com/z1Tmd8rOd9 \\u2014 MAGA Voice (@MAGAVoice) March 31, 2025\\n\\nHolding a piece of paper listing massive tariffs from the European Union, India, Japan, and Canada, the White House Press spokesperson said, \\\"If you look at the unfair trade practices that we have-- 50 per cent from the European Union on American dairy. You have a 700 per cent tariff from Japan on American rice. You have a 100 per cent tariff from India on American agricultural products. You have nearly a 300 per cent tariff from Canada on American butter and American cheese.\\\"\\n\\n\\\"This makes it virtually impossible for American products to be imported into these markets, and it puts a lot of Americans out of business and out of work over the past several decades,\\\" she added.\\n\\nCountries often heavily tariff foreign imports to protect industries or sectors vital to their own economies. President Trump's reciprocal tariffs aim to match other countries' higher tariff rates for specific goods and compensate for non-tariff barriers that put US exports at a disadvantage.\\n\\nThe Trump administration has argued that the tariff discrepancy is unfair for Americans and hurts their homegrown companies and workers.\\n\\nMs Leavitt pledged that Trump's new tariffs will bring \\\"historic change\\\" to America's trade relationships, adding, \\\"Unfortunately, these countries have been ripping off our country for far too long...And they've made their disdain, I think, for the American worker quite clear.\\\"\\n\\nTrade Barriers Report\\n\\nMeanwhile, the Trump administration on Monday also released an encyclopedic list of foreign countries' policies and regulations it regards as trade barriers, two days before the proposed hitting global trading partners with reciprocal tariffs.\\n\\nThe Office of the US Trade Representative's annual National Trade Estimate Report on foreign trade barriers lists average applied tariff rates for trading partner countries and non-tariff barriers ranging from onerous food safety regulations to renewable energy requirements and public procurement rules.\\n\\n\\\"No American President in modern history has recognized the wide-ranging and harmful foreign trade barriers American exporters face more than President Trump,\\\" US Trade Representative Jamieson Greer said in a statement.\\n\\n\\\"Under his leadership, this administration is working diligently to address these unfair and non-reciprocal practices, helping restore fairness and put hardworking American businesses and workers first in the global market,\\\" Greer added.\\n\\nIt is unclear how the 397-page report will impact Trump's reciprocal tariff plans.\\n\\nWhat Did The Report Say?\\n\\nMany of the listed trade barriers are technical in nature or the result of government regulation that blocks some U.S. exports, such as delays in EU approvals for genetically modified crops or bans on agricultural imports containing residues of certain types of pesticides.\\n\\nThe report flagged a new EU requirement for a minimum amount of post-consumer recycled content in plastic packaging as potentially creating \\\"unjustified barriers to US exports,\\\" saying the U.S. would work with the EU on the rule's implementation.\\n\\nThe report also highlighted longstanding sources of trade disputes, such as Canada's \\\"supply management\\\" system for its dairy, poultry and egg industries, which use production limits on import quotas and high tariffs, with out-of-quota tariffs on cheese at 245 per cent and butter at 298 per cent.\\n\\nIt listed VATs and their implementation as burdensome to US imports in some other countries, including Argentina, Mexico and the United Arab Emirates. The report said China's use of VAT rebates to encourage exports of certain products acted as a kind of subsidy.\\n\\n\",\n",
      "    \"url\": \"https://www.ndtv.com/world-news/100-tariffs-from-india-50-from-eu-what-white-house-said-on-donald-trumps-liberation-day-plans-8057806\"\n",
      "}\n",
      "Amid suspense over US President Donald Trump's \"Liberation Day tariff\" plans, White House spokesperson Karoline Leavitt confirmed on Monday that there won't be any exemptions on reciprocal tariffs. Listing tariffs imposed on American products by foreign nations, Ms Leavitt said that the \"unfair trade practices\" need to stop.\n",
      "\n",
      "This comes as President Trump prepares to announce a round of reciprocal tariffs on April 2, a day he has labeled \"Liberation Day\" of America.\n",
      "\n",
      "\"The goal of Wednesday is a country-based tariff, but certainly sectoral tariffs. The President has said he's committed to implementing them, and I would leave it to him when he makes that announcement,\" Leavitt said, adding that \"It's time for reciprocity.\"\n",
      "\n",
      "HOLY SH*T ðŸš¨ Karoline Leavitt brings a chart and shuts down Fake News Media with Tariff Facts:\n",
      "\n",
      "\n",
      "\n",
      "- 700% tariff from Japan on rice\n",
      "\n",
      "- 50% tariff from the EU on American dairy\n",
      "\n",
      "- 100% tariff from India on agricultural products\n",
      "\n",
      "\n",
      "\n",
      "WE COME WITH RECEIPTS pic.twitter.com/z1Tmd8rOd9 â€” MAGA Voice (@MAGAVoice) March 31, 2025\n",
      "\n",
      "Holding a piece of paper listing massive tariffs from the European Union, India, Japan, and Canada, the White House Press spokesperson said, \"If you look at the unfair trade practices that we have-- 50 per cent from the European Union on American dairy. You have a 700 per cent tariff from Japan on American rice. You have a 100 per cent tariff from India on American agricultural products. You have nearly a 300 per cent tariff from Canada on American butter and American cheese.\"\n",
      "\n",
      "\"This makes it virtually impossible for American products to be imported into these markets, and it puts a lot of Americans out of business and out of work over the past several decades,\" she added.\n",
      "\n",
      "Countries often heavily tariff foreign imports to protect industries or sectors vital to their own economies. President Trump's reciprocal tariffs aim to match other countries' higher tariff rates for specific goods and compensate for non-tariff barriers that put US exports at a disadvantage.\n",
      "\n",
      "The Trump administration has argued that the tariff discrepancy is unfair for Americans and hurts their homegrown companies and workers.\n",
      "\n",
      "Ms Leavitt pledged that Trump's new tariffs will bring \"historic change\" to America's trade relationships, adding, \"Unfortunately, these countries have been ripping off our country for far too long...And they've made their disdain, I think, for the American worker quite clear.\"\n",
      "\n",
      "Trade Barriers Report\n",
      "\n",
      "Meanwhile, the Trump administration on Monday also released an encyclopedic list of foreign countries' policies and regulations it regards as trade barriers, two days before the proposed hitting global trading partners with reciprocal tariffs.\n",
      "\n",
      "The Office of the US Trade Representative's annual National Trade Estimate Report on foreign trade barriers lists average applied tariff rates for trading partner countries and non-tariff barriers ranging from onerous food safety regulations to renewable energy requirements and public procurement rules.\n",
      "\n",
      "\"No American President in modern history has recognized the wide-ranging and harmful foreign trade barriers American exporters face more than President Trump,\" US Trade Representative Jamieson Greer said in a statement.\n",
      "\n",
      "\"Under his leadership, this administration is working diligently to address these unfair and non-reciprocal practices, helping restore fairness and put hardworking American businesses and workers first in the global market,\" Greer added.\n",
      "\n",
      "It is unclear how the 397-page report will impact Trump's reciprocal tariff plans.\n",
      "\n",
      "What Did The Report Say?\n",
      "\n",
      "Many of the listed trade barriers are technical in nature or the result of government regulation that blocks some U.S. exports, such as delays in EU approvals for genetically modified crops or bans on agricultural imports containing residues of certain types of pesticides.\n",
      "\n",
      "The report flagged a new EU requirement for a minimum amount of post-consumer recycled content in plastic packaging as potentially creating \"unjustified barriers to US exports,\" saying the U.S. would work with the EU on the rule's implementation.\n",
      "\n",
      "The report also highlighted longstanding sources of trade disputes, such as Canada's \"supply management\" system for its dairy, poultry and egg industries, which use production limits on import quotas and high tariffs, with out-of-quota tariffs on cheese at 245 per cent and butter at 298 per cent.\n",
      "\n",
      "It listed VATs and their implementation as burdensome to US imports in some other countries, including Argentina, Mexico and the United Arab Emirates. The report said China's use of VAT rebates to encourage exports of certain products acted as a kind of subsidy.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "import requests\n",
    "from newspaper.configuration import Configuration\n",
    "import json\n",
    "\n",
    "# URL of the article\n",
    "# Custom configuration to set headers\n",
    "config = Configuration()\n",
    "config.browser_user_agent = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# Create the Article object with the custom configuration\n",
    "article = Article(url, config=config)\n",
    "\n",
    "try:\n",
    "    # Fetch and parse the article\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    \n",
    "    # Extract content\n",
    "    article_data = {\n",
    "        \"title\": article.title,\n",
    "        \"authors\": article.authors,\n",
    "        \"publication_date\": str(article.publish_date) if article.publish_date else \"Unknown\",\n",
    "        \"content\": article.text,\n",
    "        \"url\": url\n",
    "    }\n",
    "\n",
    "    # Convert to JSON\n",
    "    article_json = json.dumps(article_data, indent=4)\n",
    "    print(article_json)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "article_dict = json.loads(article_json)\n",
    "\n",
    "# Extract the content part\n",
    "content = article_dict.get(\"content\", \"Content not found\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\liyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\liyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-Model News Authenticity Analysis\n",
      "===================================\n",
      "Analyzing: https://www.ndtv.com/world-news/100-tariffs-from-india-50-from-eu-what-white-house-said-on-donald-trumps-liberation-day-plans-8057806\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000235509C8720> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023516D1AE80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 968ms/step\n",
      "Individual Model Results:\n",
      "  1D_CNN: Fake (50.2%)\n",
      "LSTM_GRU: Real (50.1%)\n",
      "    BERT: Real (60.0%)\n",
      "\n",
      "Ensemble Prediction: Real (53.3% confidence)\n",
      "\n",
      "Note: This demo uses untrained models. For production use:\n",
      "- Train each model on labeled fake/real news data\n",
      "- Save model weights and tokenizers\n",
      "- Consider using pre-trained embeddings for CNN/LSTM-GRU\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM, GRU, Bidirectional\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Initialize NLP tools\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class MultiModelNewsDetector:\n",
    "    def __init__(self):\n",
    "        self.max_len = 200\n",
    "        self.max_words = 10000\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_words)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Initialize models\n",
    "        self.cnn_model = self.build_cnn_model()\n",
    "        self.lstm_gru_model = self.build_lstm_gru_model()\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def build_cnn_model(self):\n",
    "        \"\"\"1D CNN for text classification\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_words, 128, input_length=self.max_len),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def build_lstm_gru_model(self):\n",
    "        \"\"\"Hybrid LSTM-GRU model\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_words, 128, input_length=self.max_len),\n",
    "            Bidirectional(LSTM(64, return_sequences=True)),\n",
    "            GRU(64),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Standard text preprocessing\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        words = text.split()\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def extract_article(self, url):\n",
    "        \"\"\"Universal news extractor\"\"\"\n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Try multiple content selectors\n",
    "            selectors = [\n",
    "                {'class': ['article-content', 'article-body', 'content']},\n",
    "                {'itemprop': 'articleBody'},\n",
    "                {'class': re.compile('(post|entry)-content')},\n",
    "                {'id': re.compile('content|body|main')}\n",
    "            ]\n",
    "            \n",
    "            article_body = None\n",
    "            for selector in selectors:\n",
    "                article_body = soup.find('div', selector)\n",
    "                if article_body: break\n",
    "            \n",
    "            if not article_body:\n",
    "                article_body = soup  # Fallback to whole page\n",
    "            \n",
    "            paragraphs = article_body.find_all('p')\n",
    "            article_text = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])\n",
    "            \n",
    "            if len(article_text) < 100:\n",
    "                return None, \"Insufficient article text\"\n",
    "                \n",
    "            return self.clean_text(article_text), None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, f\"Extraction error: {str(e)}\"\n",
    "    \n",
    "    def prepare_input(self, text, model_type='cnn'):\n",
    "        \"\"\"Prepare input for different models\"\"\"\n",
    "        if model_type == 'bert':\n",
    "            return self.bert_tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_len,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "        else:\n",
    "            sequences = self.tokenizer.texts_to_sequences([text])\n",
    "            return pad_sequences(sequences, maxlen=self.max_len)\n",
    "    \n",
    "    def predict_with_all_models(self, url):\n",
    "        \"\"\"Run prediction with all three models\"\"\"\n",
    "        article_text, error = self.extract_article(url)\n",
    "        if error:\n",
    "            return {\"error\": error}\n",
    "        \n",
    "        # For demo, we'll fit tokenizer on the fly (in production, use pre-trained)\n",
    "        self.tokenizer.fit_on_texts([article_text])\n",
    "        \n",
    "        # CNN Prediction\n",
    "        cnn_input = self.prepare_input(article_text, 'cnn')\n",
    "        cnn_pred = self.cnn_model.predict(cnn_input)[0][0]\n",
    "        \n",
    "        # LSTM-GRU Prediction\n",
    "        lstm_gru_input = self.prepare_input(article_text, 'lstm_gru')\n",
    "        lstm_gru_pred = self.lstm_gru_model.predict(lstm_gru_input)[0][0]\n",
    "        \n",
    "        # BERT Prediction\n",
    "        bert_input = self.prepare_input(article_text, 'bert')\n",
    "        bert_output = self.bert_model(bert_input)\n",
    "        bert_pred = tf.sigmoid(bert_output.logits).numpy()[0][0]\n",
    "        \n",
    "        def format_pred(pred):\n",
    "            label = 'Real' if pred > 0.5 else 'Fake'\n",
    "            confidence = pred if pred > 0.5 else 1 - pred\n",
    "            return {'label': label, 'confidence': float(confidence * 100)}\n",
    "        \n",
    "        return {\n",
    "            'url': url,\n",
    "            'models': {\n",
    "                '1D_CNN': format_pred(cnn_pred),\n",
    "                'LSTM_GRU': format_pred(lstm_gru_pred),\n",
    "                'BERT': format_pred(bert_pred)\n",
    "            },\n",
    "            'ensemble_prediction': self.ensemble_prediction([cnn_pred, lstm_gru_pred, bert_pred])\n",
    "        }\n",
    "    \n",
    "    def ensemble_prediction(self, predictions):\n",
    "        \"\"\"Combine predictions from all models\"\"\"\n",
    "        avg_pred = np.mean(predictions)\n",
    "        label = 'Real' if avg_pred > 0.5 else 'Fake'\n",
    "        confidence = avg_pred if avg_pred > 0.5 else 1 - avg_pred\n",
    "        return {'label': label, 'confidence': float(confidence * 100)}\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    detector = MultiModelNewsDetector()\n",
    "    \n",
    "    # Example news URL (replace with actual URL)\n",
    "    test_url = url\n",
    "\n",
    "    print(\"\\nMulti-Model News Authenticity Analysis\")\n",
    "    print(\"===================================\")\n",
    "    print(f\"Analyzing: {test_url}\\n\")\n",
    "    \n",
    "    result = detector.predict_with_all_models(test_url)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    else:\n",
    "        print(\"Individual Model Results:\")\n",
    "        for model_name, pred in result['models'].items():\n",
    "            print(f\"{model_name:>8}: {pred['label']} ({pred['confidence']:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nEnsemble Prediction: {result['ensemble_prediction']['label']} \"\n",
    "              f\"({result['ensemble_prediction']['confidence']:.1f}% confidence)\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\liyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\liyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM, GRU, Bidirectional\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize NLP tools\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "class FakeNewsDetectorSystem:\n",
    "    def __init__(self, max_len=200, max_words=10000):\n",
    "        self.max_len = max_len\n",
    "        self.max_words = max_words\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_words)\n",
    "        self.models = {\n",
    "            'cnn': None,\n",
    "            'lstm_gru': None,\n",
    "            'bert': None\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Preprocess text for all models\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        words = text.split()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def load_and_prepare_data(self, true_path, fake_path, test_size=0.2):\n",
    "        \"\"\"Load and preprocess dataset\"\"\"\n",
    "        # Load data\n",
    "        true_df = pd.read_csv(true_path)\n",
    "        fake_df = pd.read_csv(fake_path)\n",
    "        \n",
    "        # Label and combine\n",
    "        true_df['label'] = 1  # Real news\n",
    "        fake_df['label'] = 0  # Fake news\n",
    "        df = pd.concat([true_df, fake_df]).sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        # Clean text\n",
    "        df['clean_text'] = df['text'].apply(self.clean_text)\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df['clean_text'], df['label'], test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Tokenize for CNN/LSTM-GRU\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "        X_train_seq = pad_sequences(self.tokenizer.texts_to_sequences(X_train), maxlen=self.max_len)\n",
    "        X_test_seq = pad_sequences(self.tokenizer.texts_to_sequences(X_test), maxlen=self.max_len)\n",
    "        \n",
    "        return X_train_seq, X_test_seq, y_train, y_test, X_train, X_test\n",
    "    \n",
    "    def build_cnn_model(self):\n",
    "        \"\"\"1D CNN architecture\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_words, 128, input_length=self.max_len),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def build_lstm_gru_model(self):\n",
    "        \"\"\"Hybrid LSTM-GRU architecture\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_words, 128, input_length=self.max_len),\n",
    "            Bidirectional(LSTM(64, return_sequences=True)),\n",
    "            GRU(64),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def train_models(self, X_train, y_train, X_test, y_test, epochs=5, batch_size=64):\n",
    "        \"\"\"Train all three model architectures\"\"\"\n",
    "        # Train CNN\n",
    "        print(\"Training CNN model...\")\n",
    "        self.models['cnn'] = self.build_cnn_model()\n",
    "        self.models['cnn'].fit(X_train, y_train, \n",
    "                              validation_data=(X_test, y_test),\n",
    "                              epochs=epochs, batch_size=batch_size)\n",
    "        \n",
    "        # Train LSTM-GRU\n",
    "        print(\"\\nTraining LSTM-GRU model...\")\n",
    "        self.models['lstm_gru'] = self.build_lstm_gru_model()\n",
    "        self.models['lstm_gru'].fit(X_train, y_train,\n",
    "                                   validation_data=(X_test, y_test),\n",
    "                                   epochs=epochs, batch_size=batch_size)\n",
    "        \n",
    "        # Initialize BERT components (fine-tuning requires separate setup)\n",
    "        print(\"\\nInitializing BERT model (requires separate fine-tuning script)\")\n",
    "        self.models['bert'] = {\n",
    "            'tokenizer': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "            'model': TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        }\n",
    "    \n",
    "    def save_models(self, save_dir='saved_models'):\n",
    "        \"\"\"Save all model components for production\"\"\"\n",
    "        import os\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save CNN model\n",
    "        save_model(self.models['cnn'], f'{save_dir}/cnn_model.h5')\n",
    "        \n",
    "        # Save LSTM-GRU model\n",
    "        save_model(self.models['lstm_gru'], f'{save_dir}/lstm_gru_model.h5')\n",
    "        \n",
    "        # Save BERT components\n",
    "        self.models['bert']['model'].save_pretrained(f'{save_dir}/bert_model')\n",
    "        self.models['bert']['tokenizer'].save_pretrained(f'{save_dir}/bert_tokenizer')\n",
    "        \n",
    "        # Save tokenizer\n",
    "        with open(f'{save_dir}/tokenizer.pkl', 'wb') as f:\n",
    "            pickle.dump(self.tokenizer, f)\n",
    "        \n",
    "        print(f\"All models saved to {save_dir} directory\")\n",
    "    \n",
    "    def load_models(self, save_dir='saved_models'):\n",
    "        \"\"\"Load pre-trained models\"\"\"\n",
    "        # Load CNN\n",
    "        self.models['cnn'] = tf.keras.models.load_model(f'{save_dir}/cnn_model.h5')\n",
    "        \n",
    "        # Load LSTM-GRU\n",
    "        self.models['lstm_gru'] = tf.keras.models.load_model(f'{save_dir}/lstm_gru_model.h5')\n",
    "        \n",
    "        # Load BERT\n",
    "        self.models['bert'] = {\n",
    "            'tokenizer': BertTokenizer.from_pretrained(f'{save_dir}/bert_tokenizer'),\n",
    "            'model': TFBertForSequenceClassification.from_pretrained(f'{save_dir}/bert_model')\n",
    "        }\n",
    "        \n",
    "        # Load tokenizer\n",
    "        with open(f'{save_dir}/tokenizer.pkl', 'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "        \n",
    "        print(\"All models loaded successfully\")\n",
    "\n",
    "# Example Usage for Training\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize system\n",
    "    detector = FakeNewsDetectorSystem()\n",
    "    \n",
    "    # Paths to your dataset (example using ISOT dataset)\n",
    "    TRUE_DATA_PATH = \"True.csv\"\n",
    "    FAKE_DATA_PATH = \"Fake.csv\"\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    X_train_seq, X_test_seq, y_train, y_test, X_train_raw, X_test_raw = detector.load_and_prepare_data(\n",
    "        TRUE_DATA_PATH, FAKE_DATA_PATH\n",
    "    )\n",
    "    \n",
    "    # Train models\n",
    "    print(\"\\nTraining models...\")\n",
    "    detector.train_models(X_train_seq, y_train, X_test_seq, y_test, epochs=3)\n",
    "    \n",
    "    # Save models for production\n",
    "    print(\"\\nSaving trained models...\")\n",
    "    detector.save_models()\n",
    "    \n",
    "    # To load models later:\n",
    "    # new_detector = FakeNewsDetectorSystem()\n",
    "    # new_detector.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "def bert_fine_tuning(train_texts, train_labels, val_texts, val_labels):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Convert data to BERT format\n",
    "    train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=200)\n",
    "    val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=200)\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(train_encodings),\n",
    "        train_labels\n",
    "    ))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(val_encodings),\n",
    "        val_labels\n",
    "    ))\n",
    "    \n",
    "    # Load and fine-tune BERT\n",
    "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_dataset.shuffle(1000).batch(16),\n",
    "              epochs=2,\n",
    "              batch_size=16,\n",
    "              validation_data=val_dataset.batch(16))\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDetectorSystem:\n",
    "    # ... (keep all existing methods) ...\n",
    "    \n",
    "    def save_models(self, save_dir='saved_models'):\n",
    "        \"\"\"Save all model components for production\"\"\"\n",
    "        import os\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save CNN model and weights\n",
    "        save_model(self.models['cnn'], f'{save_dir}/cnn_model.h5')\n",
    "        self.models['cnn'].save_weights(f'{save_dir}/cnn_weights.h5')\n",
    "        \n",
    "        # Save LSTM-GRU model and weights\n",
    "        save_model(self.models['lstm_gru'], f'{save_dir}/lstm_gru_model.h5')\n",
    "        self.models['lstm_gru'].save_weights(f'{save_dir}/lstm_gru_weights.h5')\n",
    "        \n",
    "        # Save BERT components\n",
    "        bert_dir = f'{save_dir}/bert_model'\n",
    "        os.makedirs(bert_dir, exist_ok=True)\n",
    "        self.models['bert']['model'].save_pretrained(bert_dir)\n",
    "        self.models['bert']['tokenizer'].save_pretrained(f'{save_dir}/bert_tokenizer')\n",
    "        \n",
    "        # Save tokenizer\n",
    "        with open(f'{save_dir}/tokenizer.pkl', 'wb') as f:\n",
    "            pickle.dump(self.tokenizer, f)\n",
    "        \n",
    "        print(f\"All models and weights saved to {save_dir} directory\")\n",
    "    \n",
    "    def save_weights_only(self, save_dir='saved_weights'):\n",
    "        \"\"\"Save only the weights of the models (not full architecture)\"\"\"\n",
    "        import os\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save CNN weights\n",
    "        self.models['cnn'].save_weights(f'{save_dir}/cnn_weights.h5')\n",
    "        \n",
    "        # Save LSTM-GRU weights\n",
    "        self.models['lstm_gru'].save_weights(f'{save_dir}/lstm_gru_weights.h5')\n",
    "        \n",
    "        # Save BERT weights (special handling for transformers)\n",
    "        bert_dir = f'{save_dir}/bert_weights'\n",
    "        os.makedirs(bert_dir, exist_ok=True)\n",
    "        self.models['bert']['model'].save_weights(f'{bert_dir}/bert_weights.h5')\n",
    "        \n",
    "        print(f\"Model weights saved to {save_dir} directory\")\n",
    "    \n",
    "    def load_weights_only(self, save_dir='saved_weights'):\n",
    "        \"\"\"Load only the weights into existing model architectures\"\"\"\n",
    "        # Load CNN weights\n",
    "        self.models['cnn'].load_weights(f'{save_dir}/cnn_weights.h5')\n",
    "        \n",
    "        # Load LSTM-GRU weights\n",
    "        self.models['lstm_gru'].load_weights(f'{save_dir}/lstm_gru_weights.h5')\n",
    "        \n",
    "        # Load BERT weights\n",
    "        self.models['bert']['model'].load_weights(f'{save_dir}/bert_weights/bert_weights.h5')\n",
    "        \n",
    "        print(\"All model weights loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Keep your original FakeNewsDetectorSystem for training/saving\n",
    "# 2. Add this as a new class at the end (modified slightly):\n",
    "\n",
    "class FakeNewsPredictor:\n",
    "    def __init__(self, model_dir='saved_models'):\n",
    "        \"\"\"Initialize with pre-trained models\"\"\"\n",
    "        # Load your saved models here\n",
    "        self.detector = FakeNewsDetectorSystem()\n",
    "        self.detector.load_models(model_dir)\n",
    "        \n",
    "        # Keep the web scraping and prediction methods from the new code\n",
    "        self.max_len = 200\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Add all the web scraping and prediction methods from the new code\n",
    "    def extract_article(self, url): ...\n",
    "    def predict_with_all_models(self, url): ...\n",
    "    # etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\liyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\liyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "\n",
      "Training models...\n",
      "Training CNN model...\n",
      "Epoch 1/5\n",
      "562/562 [==============================] - 69s 122ms/step - loss: 0.0892 - accuracy: 0.9635 - val_loss: 0.0174 - val_accuracy: 0.9945\n",
      "Epoch 2/5\n",
      "562/562 [==============================] - 86s 153ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.0172 - val_accuracy: 0.9937\n",
      "Epoch 3/5\n",
      "562/562 [==============================] - 87s 155ms/step - loss: 7.3922e-04 - accuracy: 0.9999 - val_loss: 0.0169 - val_accuracy: 0.9941\n",
      "Epoch 4/5\n",
      "562/562 [==============================] - 84s 150ms/step - loss: 3.8837e-04 - accuracy: 1.0000 - val_loss: 0.0170 - val_accuracy: 0.9938\n",
      "Epoch 5/5\n",
      "562/562 [==============================] - 77s 138ms/step - loss: 3.1641e-04 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 0.9941\n",
      "\n",
      "Training LSTM-GRU model...\n",
      "Epoch 1/5\n",
      "562/562 [==============================] - 321s 558ms/step - loss: 0.1004 - accuracy: 0.9634 - val_loss: 0.0569 - val_accuracy: 0.9849\n",
      "Epoch 2/5\n",
      "562/562 [==============================] - 304s 539ms/step - loss: 0.0304 - accuracy: 0.9907 - val_loss: 0.0589 - val_accuracy: 0.9816\n",
      "Epoch 3/5\n",
      "562/562 [==============================] - 279s 497ms/step - loss: 0.0115 - accuracy: 0.9965 - val_loss: 0.0329 - val_accuracy: 0.9911\n",
      "Epoch 4/5\n",
      "562/562 [==============================] - 300s 534ms/step - loss: 0.0223 - accuracy: 0.9924 - val_loss: 0.0485 - val_accuracy: 0.9850\n",
      "Epoch 5/5\n",
      "562/562 [==============================] - 275s 490ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.0271 - val_accuracy: 0.9919\n",
      "\n",
      "Initializing BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving models...\n",
      "All models saved to saved_models directory\n",
      "\n",
      "Testing prediction now....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at saved_models/bert_model were not used when initializing TFBertForSequenceClassification: ['dropout_113']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at saved_models/bert_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models loaded successfully\n",
      "\n",
      "Model Predictions:\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "CNN     : Real (confidence: 100.0%)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "LSTM_GRU: Real (confidence: 100.0%)\n",
      "BERT    : Real (confidence: 52.5%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM, GRU, Bidirectional\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "\n",
    "# Initialize NLP tools\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class FakeNewsDetector:\n",
    "    def __init__(self, max_len=200, max_words=10000):\n",
    "        self.max_len = max_len\n",
    "        self.max_words = max_words\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_words)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.models = {\n",
    "            'cnn': None,\n",
    "            'lstm_gru': None,\n",
    "            'bert': None\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Preprocess text by removing special chars, numbers, and lemmatizing\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        words = text.split()\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def load_data(self, true_path, fake_path):\n",
    "        \"\"\"Load and combine true and fake news datasets\"\"\"\n",
    "        true_df = pd.read_csv(true_path)\n",
    "        fake_df = pd.read_csv(fake_path)\n",
    "        \n",
    "        true_df['label'] = 1  # 1 for real news\n",
    "        fake_df['label'] = 0  # 0 for fake news\n",
    "        \n",
    "        df = pd.concat([true_df, fake_df]).sample(frac=1).reset_index(drop=True)\n",
    "        df['clean_text'] = df['text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_datasets(self, df, test_size=0.2):\n",
    "        \"\"\"Split data into train/test sets and tokenize\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df['clean_text'], df['label'], test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Tokenize text\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "        X_train_seq = pad_sequences(self.tokenizer.texts_to_sequences(X_train), maxlen=self.max_len)\n",
    "        X_test_seq = pad_sequences(self.tokenizer.texts_to_sequences(X_test), maxlen=self.max_len)\n",
    "        \n",
    "        return X_train_seq, X_test_seq, y_train, y_test\n",
    "    \n",
    "    def build_cnn_model(self):\n",
    "        \"\"\"Create 1D CNN model architecture\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_words, 128, input_length=self.max_len),\n",
    "            Conv1D(128, 3, activation='relu'),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def build_lstm_gru_model(self):\n",
    "        \"\"\"Create hybrid LSTM-GRU model architecture\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_words, 128, input_length=self.max_len),\n",
    "            Bidirectional(LSTM(64, return_sequences=True)),\n",
    "            GRU(64),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def initialize_bert(self):\n",
    "        \"\"\"Initialize BERT model components\"\"\"\n",
    "        self.models['bert'] = {\n",
    "            'tokenizer': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "            'model': TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        }\n",
    "    \n",
    "    def train_models(self, X_train, y_train, X_test, y_test, epochs=5, batch_size=64):\n",
    "        \"\"\"Train all three model architectures\"\"\"\n",
    "        early_stopping = EarlyStopping(patience=2, restore_best_weights=True)\n",
    "        \n",
    "        # Train CNN\n",
    "        print(\"Training CNN model...\")\n",
    "        self.models['cnn'] = self.build_cnn_model()\n",
    "        self.models['cnn'].fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Train LSTM-GRU\n",
    "        print(\"\\nTraining LSTM-GRU model...\")\n",
    "        self.models['lstm_gru'] = self.build_lstm_gru_model()\n",
    "        self.models['lstm_gru'].fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Initialize BERT (requires separate fine-tuning)\n",
    "        print(\"\\nInitializing BERT model...\")\n",
    "        self.initialize_bert()\n",
    "    \n",
    "    def save_models(self, save_dir='saved_models'):\n",
    "        \"\"\"Save all model components to disk\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save CNN model\n",
    "        save_model(self.models['cnn'], f'{save_dir}/cnn_model.h5')\n",
    "        \n",
    "        # Save LSTM-GRU model\n",
    "        save_model(self.models['lstm_gru'], f'{save_dir}/lstm_gru_model.h5')\n",
    "        \n",
    "        # Save BERT components\n",
    "        self.models['bert']['model'].save_pretrained(f'{save_dir}/bert_model')\n",
    "        self.models['bert']['tokenizer'].save_pretrained(f'{save_dir}/bert_tokenizer')\n",
    "        \n",
    "        # Save tokenizer\n",
    "        with open(f'{save_dir}/tokenizer.pkl', 'wb') as f:\n",
    "            pickle.dump(self.tokenizer, f)\n",
    "        \n",
    "        print(f\"All models saved to {save_dir} directory\")\n",
    "    \n",
    "    def load_models(self, save_dir='saved_models'):\n",
    "        \"\"\"Load pre-trained models from disk\"\"\"\n",
    "        # Load CNN\n",
    "        self.models['cnn'] = tf.keras.models.load_model(f'{save_dir}/cnn_model.h5')\n",
    "        \n",
    "        # Load LSTM-GRU\n",
    "        self.models['lstm_gru'] = tf.keras.models.load_model(f'{save_dir}/lstm_gru_model.h5')\n",
    "        \n",
    "        # Load BERT\n",
    "        self.models['bert'] = {\n",
    "            'tokenizer': BertTokenizer.from_pretrained(f'{save_dir}/bert_tokenizer'),\n",
    "            'model': TFBertForSequenceClassification.from_pretrained(f'{save_dir}/bert_model')\n",
    "        }\n",
    "        \n",
    "        # Load tokenizer\n",
    "        with open(f'{save_dir}/tokenizer.pkl', 'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "        \n",
    "        print(\"All models loaded successfully\")\n",
    "    \n",
    "    def predict(self, text, model_type='cnn'):\n",
    "        \"\"\"Make prediction on new text\"\"\"\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        if model_type == 'bert':\n",
    "            inputs = self.models['bert']['tokenizer'](\n",
    "                cleaned_text,\n",
    "                return_tensors='tf',\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_len\n",
    "            )\n",
    "            outputs = self.models['bert']['model'](inputs)\n",
    "            pred = tf.sigmoid(outputs.logits).numpy()[0][0]\n",
    "        else:\n",
    "            sequence = self.tokenizer.texts_to_sequences([cleaned_text])\n",
    "            padded_seq = pad_sequences(sequence, maxlen=self.max_len)\n",
    "            pred = self.models[model_type].predict(padded_seq)[0][0]\n",
    "        \n",
    "        return {\n",
    "            'prediction': 'Real' if pred > 0.5 else 'Fake',\n",
    "            'confidence': float(pred if pred > 0.5 else 1 - pred),\n",
    "            'raw_score': float(pred)\n",
    "        }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize detector\n",
    "    detector = FakeNewsDetector()\n",
    "    \n",
    "    # Paths to your dataset files\n",
    "    TRUE_DATA_PATH = \"True.csv\"\n",
    "    FAKE_DATA_PATH = \"Fake.csv\"\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = detector.load_data(TRUE_DATA_PATH, FAKE_DATA_PATH)\n",
    "    X_train, X_test, y_train, y_test = detector.prepare_datasets(df)\n",
    "    \n",
    "    # Train models\n",
    "    print(\"\\nTraining models...\")\n",
    "    detector.train_models(X_train, y_train, X_test, y_test, epochs=5)\n",
    "    \n",
    "    # Save models\n",
    "    print(\"\\nSaving models...\")\n",
    "    detector.save_models()\n",
    "    \n",
    "    # Example prediction\n",
    "\n",
    "    print(f\"\\nTesting prediction now....\\n\")\n",
    "    \n",
    "    # Load models (simulating a fresh start)\n",
    "    new_detector = FakeNewsDetector()\n",
    "    new_detector.load_models()\n",
    "    \n",
    "    # Make predictions with different models\n",
    "    print(\"\\nModel Predictions:\")\n",
    "    for model_name in ['cnn', 'lstm_gru', 'bert']:\n",
    "        result = new_detector.predict(test_text, model_type=model_name)\n",
    "        print(f\"{model_name.upper():<8}: {result['prediction']} (confidence: {result['confidence']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
