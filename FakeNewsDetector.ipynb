{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = input(\"ENter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"ISRO Satellite Images Reveal Massive Damage Caused By Myanmar Earthquake\",\n",
      "    \"authors\": [\n",
      "        \"Mahima Joshi\"\n",
      "    ],\n",
      "    \"publication_date\": \"Unknown\",\n",
      "    \"content\": \"ISRO Satellite Images Reveal Massive Damage Caused By Myanmar Earthquake | See Pics\\n\\nCurated By :\\n\\nNews18.com\\n\\nLast Updated: April 01, 2025, 07:19 IST\\n\\nISRO captured satellite images of the damage caused by the powerful earthquake measuring 7.7 in magnitude struck Myanmar on March 28, 2025.\\n\\nImages showing destruction caused by earthquake in Myanmar. (Image: News18)\\n\\nThe Indian Space Research Organisation (ISRO) on Monday released satellite images of the powerful earthquake caused by the 7.7 magnitude earthquake that struck Myanmar and rattled neighbouring countries on March 28, 2025.\\n\\nThe disaster caused widespread devastation, particularly near Mandalay, Myanmar\\u2019s second-largest city killing over 1,700 people, according to the latest input.\\n\\nrelated stories\\n\\nCartosat-3, the space agency\\u2019s Earth imaging satellite, which can help with images at a resolution of less than 50 centimetres, managed to capture photos from an altitude of 500 kilometres above the Earth.\\n\\nISRO Releases Satellite Images\\n\\nIn the aftermath of the earthquake, satellite imagery from ISRO\\u2019s Cartosat-3 has provided crucial insights into the extent of the destruction. The released images show how a huge bridge over the Irrawaddy River collapsed. The damage to Mandalay University was also highlighted.\\n\\nThe earthquake resulted in catastrophic damage across multiple regions, leaving several people injured, while many lost their lives. Infrastructure such as roads, residential buildings, and historical landmarks also suffered severe destruction.\\n\\nNotable damage was reported in Mandalay and the nearby Sagaing region. The images taken on March 29 revealed extensive damage to Mandalay\\u2019s infrastructure, including notable landmarks that suffered either complete or partial collapse.\\n\\nSeveral major sites like the Mahamuni Pagoda and the historic Ava Bridge also collapsed. In addition to this, tremors were felt in Thailand, prompting emergency responses there as well.\\n\\ntop videos View all Swipe Left For Next Video View all\\n\\nA state of emergency has been declared by Myanmar\\u2019s military government as rescue operations are underway to locate those trapped. According to reports, over 2,900 people have died, with thousands more injured.\\n\\nIndia was one of the first countries to offer help and fly in rescue teams to provide assistance to Myanmar at this crucial time.\\n\\nLocation : Myanmar (Burma)\\n\\nFirst Published: April 01, 2025, 07:19 IST\",\n",
      "    \"url\": \"https://www.news18.com/world/isro-satellite-images-reveal-massive-damage-caused-by-myanmar-earthquake-see-pics-9282121.html\"\n",
      "}\n",
      "ISRO Satellite Images Reveal Massive Damage Caused By Myanmar Earthquake | See Pics\n",
      "\n",
      "Curated By :\n",
      "\n",
      "News18.com\n",
      "\n",
      "Last Updated: April 01, 2025, 07:19 IST\n",
      "\n",
      "ISRO captured satellite images of the damage caused by the powerful earthquake measuring 7.7 in magnitude struck Myanmar on March 28, 2025.\n",
      "\n",
      "Images showing destruction caused by earthquake in Myanmar. (Image: News18)\n",
      "\n",
      "The Indian Space Research Organisation (ISRO) on Monday released satellite images of the powerful earthquake caused by the 7.7 magnitude earthquake that struck Myanmar and rattled neighbouring countries on March 28, 2025.\n",
      "\n",
      "The disaster caused widespread devastation, particularly near Mandalay, Myanmar’s second-largest city killing over 1,700 people, according to the latest input.\n",
      "\n",
      "related stories\n",
      "\n",
      "Cartosat-3, the space agency’s Earth imaging satellite, which can help with images at a resolution of less than 50 centimetres, managed to capture photos from an altitude of 500 kilometres above the Earth.\n",
      "\n",
      "ISRO Releases Satellite Images\n",
      "\n",
      "In the aftermath of the earthquake, satellite imagery from ISRO’s Cartosat-3 has provided crucial insights into the extent of the destruction. The released images show how a huge bridge over the Irrawaddy River collapsed. The damage to Mandalay University was also highlighted.\n",
      "\n",
      "The earthquake resulted in catastrophic damage across multiple regions, leaving several people injured, while many lost their lives. Infrastructure such as roads, residential buildings, and historical landmarks also suffered severe destruction.\n",
      "\n",
      "Notable damage was reported in Mandalay and the nearby Sagaing region. The images taken on March 29 revealed extensive damage to Mandalay’s infrastructure, including notable landmarks that suffered either complete or partial collapse.\n",
      "\n",
      "Several major sites like the Mahamuni Pagoda and the historic Ava Bridge also collapsed. In addition to this, tremors were felt in Thailand, prompting emergency responses there as well.\n",
      "\n",
      "top videos View all Swipe Left For Next Video View all\n",
      "\n",
      "A state of emergency has been declared by Myanmar’s military government as rescue operations are underway to locate those trapped. According to reports, over 2,900 people have died, with thousands more injured.\n",
      "\n",
      "India was one of the first countries to offer help and fly in rescue teams to provide assistance to Myanmar at this crucial time.\n",
      "\n",
      "Location : Myanmar (Burma)\n",
      "\n",
      "First Published: April 01, 2025, 07:19 IST\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "import requests\n",
    "from newspaper.configuration import Configuration\n",
    "import json\n",
    "\n",
    "# URL of the article\n",
    "# Custom configuration to set headers\n",
    "config = Configuration()\n",
    "config.browser_user_agent = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# Create the Article object with the custom configuration\n",
    "article = Article(url, config=config)\n",
    "\n",
    "try:\n",
    "    # Fetch and parse the article\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    \n",
    "    # Extract content\n",
    "    article_data = {\n",
    "        \"title\": article.title,\n",
    "        \"authors\": article.authors,\n",
    "        \"publication_date\": str(article.publish_date) if article.publish_date else \"Unknown\",\n",
    "        \"content\": article.text,\n",
    "        \"url\": url\n",
    "    }\n",
    "\n",
    "    # Convert to JSON\n",
    "    article_json = json.dumps(article_data, indent=4)\n",
    "    print(article_json)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "article_dict = json.loads(article_json)\n",
    "\n",
    "# Extract the content part\n",
    "content = article_dict.get(\"content\", \"Content not found\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\liyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\liyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi-Model News Authenticity Analysis\n",
      "===================================\n",
      "Analyzing: https://www.news18.com/world/isro-satellite-images-reveal-massive-damage-caused-by-myanmar-earthquake-see-pics-9282121.html\n",
      "\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 1s 903ms/step\n",
      "Individual Model Results:\n",
      "  1D_CNN: Fake (52.6%)\n",
      "LSTM_GRU: Real (50.1%)\n",
      "    BERT: Fake (50.4%)\n",
      "\n",
      "Ensemble Prediction: Fake (51.0% confidence)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM, GRU, Bidirectional\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Initialize NLP tools\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class MultiModelNewsDetector:\n",
    "    def __init__(self):\n",
    "        self.max_len = 200\n",
    "        self.max_words = 10000\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_words)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Initialize models\n",
    "        self.cnn_model = self.build_cnn_model()\n",
    "        self.lstm_gru_model = self.build_lstm_gru_model()\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    def build_cnn_model(self):\n",
    "        \"\"\"1D CNN for text classification\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_words, 128, input_length=self.max_len),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def build_lstm_gru_model(self):\n",
    "        \"\"\"Hybrid LSTM-GRU model\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_words, 128, input_length=self.max_len),\n",
    "            Bidirectional(LSTM(64, return_sequences=True)),\n",
    "            GRU(64),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Standard text preprocessing\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        words = text.split()\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def extract_article(self, url):\n",
    "        \"\"\"Universal news extractor\"\"\"\n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Try multiple content selectors\n",
    "            selectors = [\n",
    "                {'class': ['article-content', 'article-body', 'content']},\n",
    "                {'itemprop': 'articleBody'},\n",
    "                {'class': re.compile('(post|entry)-content')},\n",
    "                {'id': re.compile('content|body|main')}\n",
    "            ]\n",
    "            \n",
    "            article_body = None\n",
    "            for selector in selectors:\n",
    "                article_body = soup.find('div', selector)\n",
    "                if article_body: break\n",
    "            \n",
    "            if not article_body:\n",
    "                article_body = soup  # Fallback to whole page\n",
    "            \n",
    "            paragraphs = article_body.find_all('p')\n",
    "            article_text = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])\n",
    "            \n",
    "            if len(article_text) < 100:\n",
    "                return None, \"Insufficient article text\"\n",
    "                \n",
    "            return self.clean_text(article_text), None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, f\"Extraction error: {str(e)}\"\n",
    "    \n",
    "    def prepare_input(self, text, model_type='cnn'):\n",
    "        \"\"\"Prepare input for different models\"\"\"\n",
    "        if model_type == 'bert':\n",
    "            return self.bert_tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_len,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "        else:\n",
    "            sequences = self.tokenizer.texts_to_sequences([text])\n",
    "            return pad_sequences(sequences, maxlen=self.max_len)\n",
    "    \n",
    "    def predict_with_all_models(self, url):\n",
    "        \"\"\"Run prediction with all three models\"\"\"\n",
    "        article_text, error = self.extract_article(url)\n",
    "        if error:\n",
    "            return {\"error\": error}\n",
    "        \n",
    "        # For demo, we'll fit tokenizer on the fly (in production, use pre-trained)\n",
    "        self.tokenizer.fit_on_texts([article_text])\n",
    "        \n",
    "        # CNN Prediction\n",
    "        cnn_input = self.prepare_input(article_text, 'cnn')\n",
    "        cnn_pred = self.cnn_model.predict(cnn_input)[0][0]\n",
    "        \n",
    "        # LSTM-GRU Prediction\n",
    "        lstm_gru_input = self.prepare_input(article_text, 'lstm_gru')\n",
    "        lstm_gru_pred = self.lstm_gru_model.predict(lstm_gru_input)[0][0]\n",
    "        \n",
    "        # BERT Prediction\n",
    "        bert_input = self.prepare_input(article_text, 'bert')\n",
    "        bert_output = self.bert_model(bert_input)\n",
    "        bert_pred = tf.sigmoid(bert_output.logits).numpy()[0][0]\n",
    "        \n",
    "        def format_pred(pred):\n",
    "            label = 'Real' if pred > 0.5 else 'Fake'\n",
    "            confidence = pred if pred > 0.5 else 1 - pred\n",
    "            return {'label': label, 'confidence': float(confidence * 100)}\n",
    "        \n",
    "        return {\n",
    "            'url': url,\n",
    "            'models': {\n",
    "                '1D_CNN': format_pred(cnn_pred),\n",
    "                'LSTM_GRU': format_pred(lstm_gru_pred),\n",
    "                'BERT': format_pred(bert_pred)\n",
    "            },\n",
    "            'ensemble_prediction': self.ensemble_prediction([cnn_pred, lstm_gru_pred, bert_pred])\n",
    "        }\n",
    "    \n",
    "    def ensemble_prediction(self, predictions):\n",
    "        \"\"\"Combine predictions from all models\"\"\"\n",
    "        avg_pred = np.mean(predictions)\n",
    "        label = 'Real' if avg_pred > 0.5 else 'Fake'\n",
    "        confidence = avg_pred if avg_pred > 0.5 else 1 - avg_pred\n",
    "        return {'label': label, 'confidence': float(confidence * 100)}\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    detector = MultiModelNewsDetector()\n",
    "    \n",
    "    # Example news URL (replace with actual URL)\n",
    "    test_url = url\n",
    "\n",
    "    print(\"\\nMulti-Model News Authenticity Analysis\")\n",
    "    print(\"===================================\")\n",
    "    print(f\"Analyzing: {test_url}\\n\")\n",
    "    \n",
    "    result = detector.predict_with_all_models(test_url)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    else:\n",
    "        print(\"Individual Model Results:\")\n",
    "        for model_name, pred in result['models'].items():\n",
    "            print(f\"{model_name:>8}: {pred['label']} ({pred['confidence']:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nEnsemble Prediction: {result['ensemble_prediction']['label']} \"\n",
    "              f\"({result['ensemble_prediction']['confidence']:.1f}% confidence)\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\liyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\liyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "\n",
      "Training models...\n",
      "Training CNN model...\n",
      "Epoch 1/3\n",
      "562/562 [==============================] - 62s 109ms/step - loss: 0.0965 - accuracy: 0.9586 - val_loss: 0.0223 - val_accuracy: 0.9921\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 67s 118ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.0177 - val_accuracy: 0.9941\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 79s 140ms/step - loss: 5.9819e-04 - accuracy: 0.9999 - val_loss: 0.0197 - val_accuracy: 0.9935\n",
      "\n",
      "Training LSTM-GRU model...\n",
      "Epoch 1/3\n",
      "562/562 [==============================] - 333s 583ms/step - loss: 0.1062 - accuracy: 0.9607 - val_loss: 0.0479 - val_accuracy: 0.9863\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 252s 449ms/step - loss: 0.0228 - accuracy: 0.9931 - val_loss: 0.0441 - val_accuracy: 0.9892\n",
      "Epoch 3/3\n",
      "249/562 [============>.................] - ETA: 2:06 - loss: 0.0107 - accuracy: 0.9972"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM, GRU, Bidirectional\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize NLP tools\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "class FakeNewsDetectorSystem:\n",
    "    def __init__(self, max_len=200, max_words=10000):\n",
    "        self.max_len = max_len\n",
    "        self.max_words = max_words\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_words)\n",
    "        self.models = {\n",
    "            'cnn': None,\n",
    "            'lstm_gru': None,\n",
    "            'bert': None\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Preprocess text for all models\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        words = text.split()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def load_and_prepare_data(self, true_path, fake_path, test_size=0.2):\n",
    "        \"\"\"Load and preprocess dataset\"\"\"\n",
    "        # Load data\n",
    "        true_df = pd.read_csv(true_path)\n",
    "        fake_df = pd.read_csv(fake_path)\n",
    "        \n",
    "        # Label and combine\n",
    "        true_df['label'] = 1  # Real news\n",
    "        fake_df['label'] = 0  # Fake news\n",
    "        df = pd.concat([true_df, fake_df]).sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        # Clean text\n",
    "        df['clean_text'] = df['text'].apply(self.clean_text)\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df['clean_text'], df['label'], test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Tokenize for CNN/LSTM-GRU\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "        X_train_seq = pad_sequences(self.tokenizer.texts_to_sequences(X_train), maxlen=self.max_len)\n",
    "        X_test_seq = pad_sequences(self.tokenizer.texts_to_sequences(X_test), maxlen=self.max_len)\n",
    "        \n",
    "        return X_train_seq, X_test_seq, y_train, y_test, X_train, X_test\n",
    "    \n",
    "    def build_cnn_model(self):\n",
    "        \"\"\"1D CNN architecture\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_words, 128, input_length=self.max_len),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def build_lstm_gru_model(self):\n",
    "        \"\"\"Hybrid LSTM-GRU architecture\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_words, 128, input_length=self.max_len),\n",
    "            Bidirectional(LSTM(64, return_sequences=True)),\n",
    "            GRU(64),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def train_models(self, X_train, y_train, X_test, y_test, epochs=5, batch_size=64):\n",
    "        \"\"\"Train all three model architectures\"\"\"\n",
    "        # Train CNN\n",
    "        print(\"Training CNN model...\")\n",
    "        self.models['cnn'] = self.build_cnn_model()\n",
    "        self.models['cnn'].fit(X_train, y_train, \n",
    "                              validation_data=(X_test, y_test),\n",
    "                              epochs=epochs, batch_size=batch_size)\n",
    "        \n",
    "        # Train LSTM-GRU\n",
    "        print(\"\\nTraining LSTM-GRU model...\")\n",
    "        self.models['lstm_gru'] = self.build_lstm_gru_model()\n",
    "        self.models['lstm_gru'].fit(X_train, y_train,\n",
    "                                   validation_data=(X_test, y_test),\n",
    "                                   epochs=epochs, batch_size=batch_size)\n",
    "        \n",
    "        # Initialize BERT components (fine-tuning requires separate setup)\n",
    "        print(\"\\nInitializing BERT model (requires separate fine-tuning script)\")\n",
    "        self.models['bert'] = {\n",
    "            'tokenizer': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "            'model': TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        }\n",
    "    \n",
    "    def save_models(self, save_dir='saved_models'):\n",
    "        \"\"\"Save all model components for production\"\"\"\n",
    "        import os\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save CNN model\n",
    "        save_model(self.models['cnn'], f'{save_dir}/cnn_model.h5')\n",
    "        \n",
    "        # Save LSTM-GRU model\n",
    "        save_model(self.models['lstm_gru'], f'{save_dir}/lstm_gru_model.h5')\n",
    "        \n",
    "        # Save BERT components\n",
    "        self.models['bert']['model'].save_pretrained(f'{save_dir}/bert_model')\n",
    "        self.models['bert']['tokenizer'].save_pretrained(f'{save_dir}/bert_tokenizer')\n",
    "        \n",
    "        # Save tokenizer\n",
    "        with open(f'{save_dir}/tokenizer.pkl', 'wb') as f:\n",
    "            pickle.dump(self.tokenizer, f)\n",
    "        \n",
    "        print(f\"All models saved to {save_dir} directory\")\n",
    "    \n",
    "    def load_models(self, save_dir='saved_models'):\n",
    "        \"\"\"Load pre-trained models\"\"\"\n",
    "        # Load CNN\n",
    "        self.models['cnn'] = tf.keras.models.load_model(f'{save_dir}/cnn_model.h5')\n",
    "        \n",
    "        # Load LSTM-GRU\n",
    "        self.models['lstm_gru'] = tf.keras.models.load_model(f'{save_dir}/lstm_gru_model.h5')\n",
    "        \n",
    "        # Load BERT\n",
    "        self.models['bert'] = {\n",
    "            'tokenizer': BertTokenizer.from_pretrained(f'{save_dir}/bert_tokenizer'),\n",
    "            'model': TFBertForSequenceClassification.from_pretrained(f'{save_dir}/bert_model')\n",
    "        }\n",
    "        \n",
    "        # Load tokenizer\n",
    "        with open(f'{save_dir}/tokenizer.pkl', 'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "        \n",
    "        print(\"All models loaded successfully\")\n",
    "\n",
    "# Example Usage for Training\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize system\n",
    "    detector = FakeNewsDetectorSystem()\n",
    "    \n",
    "    # Paths to your dataset (example using ISOT dataset)\n",
    "    TRUE_DATA_PATH = \"True.csv\"\n",
    "    FAKE_DATA_PATH = \"Fake.csv\"\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    X_train_seq, X_test_seq, y_train, y_test, X_train_raw, X_test_raw = detector.load_and_prepare_data(\n",
    "        TRUE_DATA_PATH, FAKE_DATA_PATH\n",
    "    )\n",
    "    \n",
    "    # Train models\n",
    "    print(\"\\nTraining models...\")\n",
    "    detector.train_models(X_train_seq, y_train, X_test_seq, y_test, epochs=3)\n",
    "    \n",
    "    # Save models for production\n",
    "    print(\"\\nSaving trained models...\")\n",
    "    detector.save_models()\n",
    "    \n",
    "    # To load models later:\n",
    "    # new_detector = FakeNewsDetectorSystem()\n",
    "    # new_detector.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liyan\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "def bert_fine_tuning(train_texts, train_labels, val_texts, val_labels):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Convert data to BERT format\n",
    "    train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=200)\n",
    "    val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=200)\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(train_encodings),\n",
    "        train_labels\n",
    "    ))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(val_encodings),\n",
    "        val_labels\n",
    "    ))\n",
    "    \n",
    "    # Load and fine-tune BERT\n",
    "    model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_dataset.shuffle(1000).batch(16),\n",
    "              epochs=2,\n",
    "              batch_size=16,\n",
    "              validation_data=val_dataset.batch(16))\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\liyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\liyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "\n",
      "Training models...\n",
      "Training CNN model...\n",
      "Epoch 1/5\n",
      "562/562 [==============================] - 69s 122ms/step - loss: 0.0892 - accuracy: 0.9635 - val_loss: 0.0174 - val_accuracy: 0.9945\n",
      "Epoch 2/5\n",
      "562/562 [==============================] - 86s 153ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.0172 - val_accuracy: 0.9937\n",
      "Epoch 3/5\n",
      "562/562 [==============================] - 87s 155ms/step - loss: 7.3922e-04 - accuracy: 0.9999 - val_loss: 0.0169 - val_accuracy: 0.9941\n",
      "Epoch 4/5\n",
      "562/562 [==============================] - 84s 150ms/step - loss: 3.8837e-04 - accuracy: 1.0000 - val_loss: 0.0170 - val_accuracy: 0.9938\n",
      "Epoch 5/5\n",
      "562/562 [==============================] - 77s 138ms/step - loss: 3.1641e-04 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 0.9941\n",
      "\n",
      "Training LSTM-GRU model...\n",
      "Epoch 1/5\n",
      "562/562 [==============================] - 321s 558ms/step - loss: 0.1004 - accuracy: 0.9634 - val_loss: 0.0569 - val_accuracy: 0.9849\n",
      "Epoch 2/5\n",
      "562/562 [==============================] - 304s 539ms/step - loss: 0.0304 - accuracy: 0.9907 - val_loss: 0.0589 - val_accuracy: 0.9816\n",
      "Epoch 3/5\n",
      "562/562 [==============================] - 279s 497ms/step - loss: 0.0115 - accuracy: 0.9965 - val_loss: 0.0329 - val_accuracy: 0.9911\n",
      "Epoch 4/5\n",
      "562/562 [==============================] - 300s 534ms/step - loss: 0.0223 - accuracy: 0.9924 - val_loss: 0.0485 - val_accuracy: 0.9850\n",
      "Epoch 5/5\n",
      "562/562 [==============================] - 275s 490ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.0271 - val_accuracy: 0.9919\n",
      "\n",
      "Initializing BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving models...\n",
      "All models saved to saved_models directory\n",
      "\n",
      "Testing prediction now....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at saved_models/bert_model were not used when initializing TFBertForSequenceClassification: ['dropout_113']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at saved_models/bert_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models loaded successfully\n",
      "\n",
      "Model Predictions:\n",
      "1/1 [==============================] - 0s 214ms/step\n",
      "CNN     : Real (confidence: 100.0%)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "LSTM_GRU: Real (confidence: 100.0%)\n",
      "BERT    : Real (confidence: 52.5%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM, GRU, Bidirectional\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "\n",
    "# Initialize NLP tools\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class FakeNewsDetector:\n",
    "    def __init__(self, max_len=200, max_words=10000):\n",
    "        self.max_len = max_len\n",
    "        self.max_words = max_words\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_words)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.models = {\n",
    "            'cnn': None,\n",
    "            'lstm_gru': None,\n",
    "            'bert': None\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Preprocess text by removing special chars, numbers, and lemmatizing\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        words = text.split()\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def load_data(self, true_path, fake_path):\n",
    "        \"\"\"Load and combine true and fake news datasets\"\"\"\n",
    "        true_df = pd.read_csv(true_path)\n",
    "        fake_df = pd.read_csv(fake_path)\n",
    "        \n",
    "        true_df['label'] = 1  # 1 for real news\n",
    "        fake_df['label'] = 0  # 0 for fake news\n",
    "        \n",
    "        df = pd.concat([true_df, fake_df]).sample(frac=1).reset_index(drop=True)\n",
    "        df['clean_text'] = df['text'].apply(self.clean_text)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_datasets(self, df, test_size=0.2):\n",
    "        \"\"\"Split data into train/test sets and tokenize\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df['clean_text'], df['label'], test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Tokenize text\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "        X_train_seq = pad_sequences(self.tokenizer.texts_to_sequences(X_train), maxlen=self.max_len)\n",
    "        X_test_seq = pad_sequences(self.tokenizer.texts_to_sequences(X_test), maxlen=self.max_len)\n",
    "        \n",
    "        return X_train_seq, X_test_seq, y_train, y_test\n",
    "    \n",
    "    def build_cnn_model(self):\n",
    "        \"\"\"Create 1D CNN model architecture\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_words, 128, input_length=self.max_len),\n",
    "            Conv1D(128, 3, activation='relu'),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def build_lstm_gru_model(self):\n",
    "        \"\"\"Create hybrid LSTM-GRU model architecture\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_words, 128, input_length=self.max_len),\n",
    "            Bidirectional(LSTM(64, return_sequences=True)),\n",
    "            GRU(64),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def initialize_bert(self):\n",
    "        \"\"\"Initialize BERT model components\"\"\"\n",
    "        self.models['bert'] = {\n",
    "            'tokenizer': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "            'model': TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        }\n",
    "    \n",
    "    def train_models(self, X_train, y_train, X_test, y_test, epochs=5, batch_size=64):\n",
    "        \"\"\"Train all three model architectures\"\"\"\n",
    "        early_stopping = EarlyStopping(patience=2, restore_best_weights=True)\n",
    "        \n",
    "        # Train CNN\n",
    "        print(\"Training CNN model...\")\n",
    "        self.models['cnn'] = self.build_cnn_model()\n",
    "        self.models['cnn'].fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Train LSTM-GRU\n",
    "        print(\"\\nTraining LSTM-GRU model...\")\n",
    "        self.models['lstm_gru'] = self.build_lstm_gru_model()\n",
    "        self.models['lstm_gru'].fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Initialize BERT (requires separate fine-tuning)\n",
    "        print(\"\\nInitializing BERT model...\")\n",
    "        self.initialize_bert()\n",
    "    \n",
    "    def save_models(self, save_dir='saved_models'):\n",
    "        \"\"\"Save all model components to disk\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save CNN model\n",
    "        save_model(self.models['cnn'], f'{save_dir}/cnn_model.h5')\n",
    "        \n",
    "        # Save LSTM-GRU model\n",
    "        save_model(self.models['lstm_gru'], f'{save_dir}/lstm_gru_model.h5')\n",
    "        \n",
    "        # Save BERT components\n",
    "        self.models['bert']['model'].save_pretrained(f'{save_dir}/bert_model')\n",
    "        self.models['bert']['tokenizer'].save_pretrained(f'{save_dir}/bert_tokenizer')\n",
    "        \n",
    "        # Save tokenizer\n",
    "        with open(f'{save_dir}/tokenizer.pkl', 'wb') as f:\n",
    "            pickle.dump(self.tokenizer, f)\n",
    "        \n",
    "        print(f\"All models saved to {save_dir} directory\")\n",
    "    \n",
    "    def load_models(self, save_dir='saved_models'):\n",
    "        \"\"\"Load pre-trained models from disk\"\"\"\n",
    "        # Load CNN\n",
    "        self.models['cnn'] = tf.keras.models.load_model(f'{save_dir}/cnn_model.h5')\n",
    "        \n",
    "        # Load LSTM-GRU\n",
    "        self.models['lstm_gru'] = tf.keras.models.load_model(f'{save_dir}/lstm_gru_model.h5')\n",
    "        \n",
    "        # Load BERT\n",
    "        self.models['bert'] = {\n",
    "            'tokenizer': BertTokenizer.from_pretrained(f'{save_dir}/bert_tokenizer'),\n",
    "            'model': TFBertForSequenceClassification.from_pretrained(f'{save_dir}/bert_model')\n",
    "        }\n",
    "        \n",
    "        # Load tokenizer\n",
    "        with open(f'{save_dir}/tokenizer.pkl', 'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "        \n",
    "        print(\"All models loaded successfully\")\n",
    "    \n",
    "    def predict(self, text, model_type='cnn'):\n",
    "        \"\"\"Make prediction on new text\"\"\"\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        if model_type == 'bert':\n",
    "            inputs = self.models['bert']['tokenizer'](\n",
    "                cleaned_text,\n",
    "                return_tensors='tf',\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_len\n",
    "            )\n",
    "            outputs = self.models['bert']['model'](inputs)\n",
    "            pred = tf.sigmoid(outputs.logits).numpy()[0][0]\n",
    "        else:\n",
    "            sequence = self.tokenizer.texts_to_sequences([cleaned_text])\n",
    "            padded_seq = pad_sequences(sequence, maxlen=self.max_len)\n",
    "            pred = self.models[model_type].predict(padded_seq)[0][0]\n",
    "        \n",
    "        return {\n",
    "            'prediction': 'Real' if pred > 0.5 else 'Fake',\n",
    "            'confidence': float(pred if pred > 0.5 else 1 - pred),\n",
    "            'raw_score': float(pred)\n",
    "        }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize detector\n",
    "    detector = FakeNewsDetector()\n",
    "    \n",
    "    # Paths to your dataset files\n",
    "    TRUE_DATA_PATH = \"True.csv\"\n",
    "    FAKE_DATA_PATH = \"Fake.csv\"\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = detector.load_data(TRUE_DATA_PATH, FAKE_DATA_PATH)\n",
    "    X_train, X_test, y_train, y_test = detector.prepare_datasets(df)\n",
    "    \n",
    "    # Train models\n",
    "    print(\"\\nTraining models...\")\n",
    "    detector.train_models(X_train, y_train, X_test, y_test, epochs=5)\n",
    "    \n",
    "    # Save models\n",
    "    print(\"\\nSaving models...\")\n",
    "    detector.save_models()\n",
    "    \n",
    "    # Example prediction\n",
    "\n",
    "    print(f\"\\nTesting prediction now....\\n\")\n",
    "    \n",
    "    # Load models (simulating a fresh start)\n",
    "    new_detector = FakeNewsDetector()\n",
    "    new_detector.load_models()\n",
    "    \n",
    "    # Make predictions with different models\n",
    "    print(\"\\nModel Predictions:\")\n",
    "    for model_name in ['cnn', 'lstm_gru', 'bert']:\n",
    "        result = new_detector.predict(test_text, model_type=model_name)\n",
    "        print(f\"{model_name.upper():<8}: {result['prediction']} (confidence: {result['confidence']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
